## 一、无监督学习概述

+ #### 什么是无监督学习

  之所以称为无监督，是因为模型学习是从无标签的数据开始学习的。

+ #### 无监督学习包含算法

  - 聚类
    - K-means(K均值聚类)
  - 降维
    - PCA

## 二、K-means原理

+ #### K-means聚类步骤

  1. 随机设置K个特征空间内的点作为初始的聚类中心
  2. 对于其他每个点计算到K个中心的距离，未知的点选择最近的一个聚类中心点作为标记类别
  3. 接着对着标记的聚类中心之后，重新计算出每个聚类的新中心点（平均值）
  4. 如果计算得出的新中心点与原中心点一样，那么结束，否则重新进行第二步过程

+ #### 图片助解

  ![](\img\K-means过程分析.png)

## 三、K-means - API

sklearn.cluster.KMeans(n_clusters=8,init=‘k-means++’)

- k-means聚类
- n_clusters
  - 开始的聚类中心数量
- init
  - 初始化方法，默认为'k-means ++’
- labels_
  - 默认标记的类型，可以和真实值比较（不是值比较）

## 四、K-means性能评估指标

#### 1. 轮廓系数

​	
$$
sc_i = \frac{b_i-a_i}{max(b_i,a_i)}
$$

> 注：
>
> + 对于每个 i 点是已聚类数据中的样本点 ，$b_i$ 为 i 点到其它簇中所有样本点的距离的最小值，$a_i$ 为 i 到自身簇的所有样本点的距离的平均值。最终计算出所有的样本点的轮廓系数平均值

#### 2. 轮廓系数值分析

- 根据公式极端值考虑：
  - 如果 $b_i >>a_i$ 那么公式结果趋近于 1，效果好。
  - 如果 $a_i>>b_i$ 那么公式结果趋近于 -1，效果不好。
- 轮廓系数的值是介于 [-1,1] ，越趋近于1代表内聚度和分离度都相对较优。

#### 3. 轮廓系数 - API 

sklearn.metrics.silhouette_score(X, labels)

- 计算所有样本的平均轮廓系数
- X
  - 特征值
- labels
  - 被聚类标记的目标值



未完待续....