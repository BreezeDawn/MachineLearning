---
title: MachineLearning-聚类算法
tags:
  - 机器学习
  - 聚类
categories:
  - 机器学习
abbrlink: 37668
date: 2018-11-08 22:33:48
---

在无监督学习中，我们的训练集可以写成只有$x^{(1)}$,$x^{(2)}$…..一直到$x^{(m)}$。我们没有任何标签 $y$。

我们希望有一种算法能够自动的把这些数据分成有紧密关系的子集或是簇。

<!-- more -->

## K-均值算法(K-Means)

#### 算法步骤综述

K-均值是一个迭代算法，假设我们想要将数据聚类成n个组，其方法为: 

1. 首先选择k个随机的点，称为聚类中心(cluster centroids);
2. 簇分配(cluster assignment) 对于数据集中的每一个数据，按照距离K个中心点的距离，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类。
3. 移动聚类中心(move centroids) 计算 每一个组 的平均值，将该组所关联的中心点移动到平均值的位置。
4. 重复步骤 2-4 直至中心点不再变化。

#### 定义损失函数变量

1. 假设有K个簇，$c^{(i)}$表示样本$x^{(i)}$ 当前所属的簇的索引编号 ，$c^{(i)}∈(1,2,3...K)$
2. $μ_k$ 表示 第k个聚类中心 的位置，其中 $k∈1,2,3,4...K$
3. 根据以上定义:则$μ_c(i)$ 表示样本$x^(i)$所属簇的中心的 位置坐标

#### K-means算法的优化目标

损失函数为 每个样本到其所属簇的中心的距离和的平均值 ，优化函数的输入参数为 每个样本所属的簇的编号$c^{(i)}$和每个簇中心的坐标$μ_k$ 这两个都是在聚类过程中不断变化的变量。此代价函数也被称为 畸变函数(Distortion function) 

#### K-means算法步骤与优化函数

+ 对于K-means算法中的 簇分配(将每个样本点分配到距离最近的簇) 的步骤实际上就是在最小化代价函数 $J$，即在$μ_1,μ_2,μ_3,μ_4...μ_K$固定的条件下调整 $c^{(1)},c^{(2)},c^{(3)},...c^{(m)}$的值以使损失函数的值最小。
+ 对于K-means算法中的 移动聚类中心(将聚类中心移动到分配样本簇的平均值处) ，即在$c^{(1)},c^{(2)},c^{(3)},...c^{(m)}$固定的条件下调整 $μ_1,μ_2,μ_3,μ_4...μ_K$的值以使损失函数的值最小。

## K均值算法簇中心的随机初始化 Random initialization

#### 随机初始化遵循法则

+ 我们应该选择 K小于m，即聚类中心点的个数要小于所有训练集实例的数量
+ 随机选择 K 个训练实例，然后令 K 个聚类中心分别与这 K 个训练实例相等

#### 随机初始化的局限性

+ 随机初始化很容易把 初始化簇中心 分到相近的样本中，这种初始化方式有其局限性。 
+ K-均值的一个问题在于，**它有可能会停留在一个局部最小值处，而这取决于初始化的情况。**



#### 改进初始化方式–多次随机初始化

+ 假如随机初始化K-means算法100 (一般是50-1000) 次之间，每次都使用不同的随机初始化方式，然后运行K-means算法，得到100种不同的聚类方式，都计算其损失函数，选取代价最小的聚类方式作为最终的聚类方式。
+ 这种方法在 K 较小的时候（2–10）还是可行的，但是如果 K 较大，这么做也可能不会有明显地改善。(不同初始化方式得到的结果趋于一致)

## K均值算法聚类数K的选择 Choosing the Number of Cluters

+ 没有所谓最好的选择聚类数的方法，通常是需要根据不同的问题，人工进行选择的。选择的时候思考我们运用 K-均值算法聚类的动机是什么，然后选择能最好服务于该目的标聚类数。

#### 肘部法则(Elbow method)

+ 改变聚类数K，然后进行聚类，计算损失函数，拐点处即为推荐的聚类数 (即通过此点后，聚类数的增大也不会对损失函数的下降带来很大的影响，所以会选择拐点) 
+ 但是也有损失函数随着K的增大平缓下降的例子，此时通过肘部法则选择K的值就不是一个很有效的方法了(下图中的拐点不明显，k=3,4,5有类似的功能)

#### 目标法则

+ 通常K均值聚类是为下一步操作做准备，例如：**市场分割，社交网络分析，网络集群优化** ，下一步的操作都能给你一些评价指标，那么决定聚类的数量更好的方式是：**看哪个聚类数量能更好的应用于后续目的**